FINAL MODEL SUMMARY (Titanic Survival Prediction)

--------------------------------------------------

Decision Tree

Best Parameters:
criterion = gini
max_depth = 4
min_samples_leaf = 20
min_samples_split = 20
max_features = sqrt
class_weight = balanced
random_state = 42

Best Cross-Validation Accuracy: 0.8216

--------------------------------------------------

K-Nearest Neighbors (KNN)

Best Parameters:
n_neighbors = 19

Mean Cross-Validation Accuracy: 0.8126
Standard Deviation: 0.0125

--------------------------------------------------

LightGBM

Best Parameters:
objective = binary
boosting_type = gbdt
n_estimators = 1000
learning_rate = 0.03
max_depth = 5
num_leaves = 7
min_child_samples = 50
subsample = 0.7
colsample_bytree = 0.9
reg_alpha = 0.5
reg_lambda = 1.0
random_state = 42

Best Cross-Validation Accuracy: 0.8385

--------------------------------------------------

Logistic Regression

Best Parameters:
C = 0.5
penalty = l2
solver = liblinear
max_iter = 1000
class_weight = balanced
random_state = 42

Best Cross-Validation Accuracy: 0.8230

--------------------------------------------------

Random Forest

Best Parameters:
n_estimators = 400
max_features = 0.5
min_samples_leaf = 5
min_samples_split = 2
bootstrap = True
class_weight = balanced
random_state = 42

Best Cross-Validation Accuracy: 0.8188

--------------------------------------------------

XGBoost

Best Parameters:
n_estimators = 300
learning_rate = 0.05
max_depth = 4
min_child_weight = 5
gamma = 0.3
subsample = 0.8
colsample_bytree = 0.8
reg_alpha = 0.0
reg_lambda = 1.0
objective = binary:logistic
eval_metric = logloss
random_state = 42

Best Cross-Validation Accuracy: 0.8455

--------------------------------------------------

Final Accuracy Ranking:

1. XGBoost – 0.8455
2. LightGBM – 0.8385
3. Logistic Regression – 0.8230
4. Decision Tree – 0.8216
5. Random Forest – 0.8188
6. KNN – 0.8126